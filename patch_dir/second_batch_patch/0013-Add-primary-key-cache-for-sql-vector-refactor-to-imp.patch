From 9938307b0bc6d7a073921e3f751afb07d8da233f Mon Sep 17 00:00:00 2001
From: Jianmei Zhang <jianmeiz@moqi.ai>
Date: Mon, 30 Jan 2023 05:31:13 +0000
Subject: [PATCH 13/85] Add primary key cache for sql + vector refactor to
 improve QPS

---
 .../QueryPlan/ReadWithVectorScan.cpp          |  30 +-
 src/Processors/QueryPlan/ReadWithVectorScan.h |   2 +-
 ...MergeTreeSelectWithVectorScanProcessor.cpp | 321 ++++++++++++++++--
 .../MergeTreeSelectWithVectorScanProcessor.h  |  11 +-
 .../MergeTree/PrimaryKeyCacheManager.cpp      |  33 +-
 .../MergeTree/PrimaryKeyCacheManager.h        |   9 +-
 ...s_lightweight_delete_with_vector.reference |   1 +
 ...16_mqvs_lightweight_delete_with_vector.sql |   2 +
 ...1_mqvs_support_primary_key_cache.reference |  12 +
 .../00021_mqvs_support_primary_key_cache.sql  |  18 +
 10 files changed, 371 insertions(+), 68 deletions(-)
 create mode 100644 tests/queries/2_vector_search/00021_mqvs_support_primary_key_cache.reference
 create mode 100644 tests/queries/2_vector_search/00021_mqvs_support_primary_key_cache.sql

diff --git a/src/Processors/QueryPlan/ReadWithVectorScan.cpp b/src/Processors/QueryPlan/ReadWithVectorScan.cpp
index cef411f3d8..f467375acd 100644
--- a/src/Processors/QueryPlan/ReadWithVectorScan.cpp
+++ b/src/Processors/QueryPlan/ReadWithVectorScan.cpp
@@ -2,6 +2,7 @@
 #include <Parsers/ASTSelectQuery.h>
 #include <Processors/QueryPlan/ReadFromMergeTree.h>
 #include <Processors/QueryPlan/ReadWithVectorScan.h>
+#include <Processors/Sources/NullSource.h>
 #include <QueryPipeline/QueryPipelineBuilder.h>
 #include <Storages/MergeTree/MergeTreeVectorScanManager.h>
 #include <Storages/MergeTree/MergeTreeSelectWithVectorScanProcessor.h>
@@ -86,6 +87,8 @@ void ReadWithVectorScan::initializePipeline(QueryPipelineBuilder & pipeline, con
     /// If there are only virtual columns in the query, should be wrong, just return.
     if (column_names_to_read.empty())
     {
+        LOG_DEBUG(log, "column_names_to_read is empty");
+        pipeline.init(Pipe(std::make_shared<NullSource>(getOutputStream().header)));
         return;
     }
 
@@ -93,6 +96,12 @@ void ReadWithVectorScan::initializePipeline(QueryPipelineBuilder & pipeline, con
         prepared_parts,
         column_names_to_read);
 
+    if (pipe.empty())
+    {
+        pipeline.init(Pipe(std::make_shared<NullSource>(getOutputStream().header)));
+        return;
+    }
+
     for (const auto & processor : pipe.getProcessors())
     {
         LOG_DEBUG(log, "[initializePipeline] add processor: {}", processor->getName());
@@ -113,21 +122,28 @@ Pipe ReadWithVectorScan::createReadProcessorsAmongParts(
         return {};
 
     const auto & settings = context->getSettingsRef();
-    const size_t min_parts_per_stream = (parts.size() - 1) / requested_num_streams + 1;
-    
+
     Pipes res;
 
-    for (size_t i = 0; i < requested_num_streams && !parts.empty(); ++i)
+    size_t num_streams = requested_num_streams;
+    if (num_streams > 1)
+    {
+        /// Reduce the number of num_streams if the data is small.
+        if (parts.size() < num_streams)
+            num_streams = parts.size();
+    }
+
+    const size_t min_parts_per_stream = (parts.size() - 1) / num_streams + 1;
+    for (size_t i = 0; i < num_streams && !parts.empty(); ++i)
     {
         MergeTreeData::DataPartsVector new_parts;
-        size_t need_parts = min_parts_per_stream;
-        while (need_parts > 0 && !parts.empty())
+        for (size_t need_parts = min_parts_per_stream; need_parts > 0 && !parts.empty(); need_parts--)
         {
             new_parts.push_back(parts.back());
             parts.pop_back();
         }
 
-        res.emplace_back(readFromParts(new_parts, column_names, settings.use_uncompressed_cache));
+        res.emplace_back(readFromParts(std::move(new_parts), column_names, settings.use_uncompressed_cache));
     }
 
     auto pipe = Pipe::unitePipes(std::move(res));
@@ -136,7 +152,7 @@ Pipe ReadWithVectorScan::createReadProcessorsAmongParts(
 }
 
 Pipe ReadWithVectorScan::readFromParts(
-    MergeTreeData::DataPartsVector & parts,
+    const MergeTreeData::DataPartsVector & parts,
     Names required_columns,
     bool use_uncompressed_cache)
 {
diff --git a/src/Processors/QueryPlan/ReadWithVectorScan.h b/src/Processors/QueryPlan/ReadWithVectorScan.h
index 7fff5a4e33..e3a92409b2 100644
--- a/src/Processors/QueryPlan/ReadWithVectorScan.h
+++ b/src/Processors/QueryPlan/ReadWithVectorScan.h
@@ -62,7 +62,7 @@ private:
     Poco::Logger * log;
 
     Pipe readFromParts(
-        MergeTreeData::DataPartsVector & parts,
+        const MergeTreeData::DataPartsVector & parts,
         Names required_columns,
         bool use_uncompressed_cache);
 };
diff --git a/src/Storages/MergeTree/MergeTreeSelectWithVectorScanProcessor.cpp b/src/Storages/MergeTree/MergeTreeSelectWithVectorScanProcessor.cpp
index 12725efd76..7a907c2f96 100644
--- a/src/Storages/MergeTree/MergeTreeSelectWithVectorScanProcessor.cpp
+++ b/src/Storages/MergeTree/MergeTreeSelectWithVectorScanProcessor.cpp
@@ -15,6 +15,34 @@ namespace ErrorCodes
     extern const int MEMORY_LIMIT_EXCEEDED;
 }
 
+static bool isVectorSearchByPk(const std::vector<String> & pk_col_names, const std::vector<String> & read_col_names)
+{
+    size_t pk_col_nums = pk_col_names.size();
+    size_t read_col_nums = read_col_names.size();
+
+    /// Currently primary key cache support only one column in PK.
+    if (read_col_nums <= pk_col_nums || pk_col_nums > 1)
+        return false;
+
+    const String pk_col_name = pk_col_names[0];
+
+    /// Read columns can only be primary key columns, or vector scan functions, distance, batch_distance.
+    bool match = true;
+    for (const auto & read_col_name : read_col_names)
+    {
+        if ((read_col_name == pk_col_name) || isVectorScanFunc(read_col_name))
+            continue;
+        else
+        {
+            match = false;
+            break;
+        }
+    }
+
+    return match;
+}
+
+/// Referenced from MergeTreeSelectProcessor::initializeReaders() and MergeTreeBaseSelectProcessor::initializeMergeTreeReadersForPart()
 void MergeTreeSelectWithVectorScanProcessor::initializeReadersWithVectorScan()
 {
     OpenTelemetrySpanHolder span("MergeTreeSelectWithVectorScanProcessor::initializeReadersWithVectorScan()");
@@ -136,6 +164,108 @@ ColumnPtr MergeTreeSelectWithVectorScanProcessor::performPrefilter(MarkRanges &
     return new_filter;
 }
 
+
+bool MergeTreeSelectWithVectorScanProcessor::readPrimaryKeyBin(Columns & out_columns)
+{
+    const KeyDescription & primary_key = storage_snapshot->metadata->getPrimaryKey();
+    const size_t pk_columns_size = primary_key.column_names.size();
+
+    NamesAndTypesList cols;
+    const std::vector<String> pk_column_names = primary_key.column_names;
+    for (const String & col_name : pk_column_names)
+    {
+        std::optional<NameAndTypePair> column_with_type = storage_snapshot->metadata->getColumns().getAllPhysical().tryGetByName(col_name);
+        if (column_with_type)
+            cols.emplace_back(*column_with_type);
+    }
+    const size_t cols_size = cols.size();
+
+    if (pk_columns_size == 0 || pk_columns_size != cols_size)
+    {
+        LOG_ERROR(log, "[readPrimaryKeyBin]: pk_columns_size = {}, cols_size = {}", pk_columns_size, cols_size);
+        return false;
+    }
+
+    LOG_DEBUG(log, "[readPrimaryKeyBin]: pk_columns_size = {}", pk_columns_size);
+
+    MutableColumns buffered_columns;
+    buffered_columns.resize(cols_size);
+    for (size_t i = 0; i < cols_size; ++i)
+    {
+        buffered_columns[i] = primary_key.data_types[i]->createColumn();
+    }
+
+    MergeTreeReaderPtr reader = task->data_part->getReader(
+            cols,
+            storage_snapshot->metadata,
+            MarkRanges{MarkRange(0, task->data_part->getMarksCount())},
+            nullptr,
+            storage.getContext()->getMarkCache().get(),
+            reader_settings);
+
+    if (!reader)
+    {
+        LOG_ERROR(log, "[readPrimaryKeyBin]: make reader failed");
+        return false;
+    }
+
+    /// begin to read
+    const MergeTreeIndexGranularity & index_granularity = task->data_part->index_granularity;
+
+    size_t current_mark = 0;
+    const size_t total_mark = task->data_part->getMarksCount();
+
+    size_t num_rows_read = 0;
+    const size_t num_rows_total = task->data_part->rows_count;
+
+    LOG_DEBUG(log, "[readPrimaryKeyBin]: total_mark = {}", total_mark);
+    LOG_DEBUG(log, "[readPrimaryKeyBin]: num_rows_total = {}", num_rows_total);
+
+    bool continue_read = false;
+
+    while (num_rows_read < num_rows_total)
+    {
+        size_t remaining_size = num_rows_total - num_rows_read;
+
+        Columns result;
+        result.resize(cols_size);
+
+        size_t num_rows = reader->readRows(current_mark, 0, continue_read, remaining_size, result);
+
+        continue_read = true;
+        num_rows_read += num_rows;
+
+        for (size_t i = 0; i < cols_size; ++i)
+        {
+            buffered_columns[i]->insertRangeFrom(*result[i], 0, result[i]->size());
+        }
+
+        /// calculate next mark
+        for (size_t mark = 0; mark < total_mark - 1; ++mark)
+        {
+            if (index_granularity.getMarkStartingRow(mark) >= num_rows_read
+                && index_granularity.getMarkStartingRow(mark + 1) < num_rows_read)
+            {
+                current_mark = mark;
+            }
+        }
+    }
+
+    for (auto & buffered_column : buffered_columns)
+    {
+        buffered_column->protect();
+    }
+
+    LOG_DEBUG(log, "[readPrimaryKeyBin]: finally, {} rows has been read", buffered_columns[0]->size());
+
+    out_columns.assign(
+        std::make_move_iterator(buffered_columns.begin()),
+        std::make_move_iterator(buffered_columns.end())
+        );
+
+    return true;
+}
+
 MergeTreeBaseSelectProcessor::BlockAndRowCount MergeTreeSelectWithVectorScanProcessor::readFromPart()
 {
     OpenTelemetrySpanHolder span("MergeTreeSelectWithVectorScanProcessor::readFromPart()");
@@ -145,21 +275,39 @@ MergeTreeBaseSelectProcessor::BlockAndRowCount MergeTreeSelectWithVectorScanProc
         bool last_reader = false;
         size_t pre_readers_shift = 0;
 
-        /// Add _part_offset to non_const_virtual_column_names if has vector_scan_manager and no prewhere_info
-        bool found = false;
-        for (const auto & column_name : non_const_virtual_column_names)
+        /// Initialize primary key cache
+        const auto & primary_key = storage_snapshot->metadata->getPrimaryKey();
+        const bool enable_primary_key_cache = task->data_part->storage.getSettings()->enable_primary_key_cache.value;
+        LOG_DEBUG(log, "[initializeRangeReaders] reader setting: enable_primary_key_cache = {}", enable_primary_key_cache);
+
+        /// consider cache if and only if
+        /// 1. this task is vector search and no prewhere info
+        /// 2. primary key is only a column, and select columns are (pk, distance) or (pk, batch_distance)
+        /// 3. primary key's type is UInt32 or UInt64
+        if (enable_primary_key_cache)
         {
-            if (column_name == "_part_offset")
-            {
-                found = true;
-                break;
-            }
+            use_primary_key_cache = PrimaryKeyCacheManager::isSupportedPrimaryKey(primary_key)
+                                    && isVectorSearchByPk(primary_key.column_names, task->ordered_names);
         }
 
-        if (!found)
+        /// Add _part_offset to non_const_virtual_column_names if part has lightweight delete
+        if (task->data_part->hasLightweightDelete())
         {
-            non_const_virtual_column_names.emplace_back("_part_offset");
-            need_remove_part_offset = true;
+            bool found = false;
+            for (const auto & column_name : non_const_virtual_column_names)
+            {
+                if (column_name == "_part_offset")
+                {
+                    found = true;
+                    break;
+                }
+            }
+
+            if (!found)
+            {
+                non_const_virtual_column_names.emplace_back("_part_offset");
+                need_remove_part_offset = true;
+            }
         }
 
         /// Add filtering step with lightweight delete mask
@@ -179,6 +327,7 @@ MergeTreeBaseSelectProcessor::BlockAndRowCount MergeTreeSelectWithVectorScanProc
 }
 
 /// perform actual read and result merge operation, prewhere has been processed ahead
+/// Referenced from MergeTreeBaseSelectProcessor::readFromPartImpl()
 MergeTreeBaseSelectProcessor::BlockAndRowCount MergeTreeSelectWithVectorScanProcessor::readFromPartWithVectorScan()
 {
     OpenTelemetrySpanHolder span("MergeTreeSelectWithVectorScanProcessor::readFromPartWithVectorScan()");
@@ -190,11 +339,132 @@ MergeTreeBaseSelectProcessor::BlockAndRowCount MergeTreeSelectWithVectorScanProc
     auto read_start_time = std::chrono::system_clock::now();
     UInt64 rows_to_read = std::max(UInt64(1), current_max_block_size_rows);
 
-    LOG_DEBUG(log, "[readFromPartImpl] begin read, mark_ranges size = {}", task->mark_ranges.size());
+    if (use_primary_key_cache)
+    {
+        LOG_DEBUG(log, "[readFromPartWithVectorScan] use primary key cache");
+
+        const String cache_key = task->data_part->getFullRelativePath() + ":" + task->data_part->name;
+
+        std::optional<Columns> pk_cache_cols_opt = PrimaryKeyCacheManager::getMgr().getPartPkCache(cache_key);
+
+        if (pk_cache_cols_opt.has_value())
+        {
+            LOG_DEBUG(log, "[readFromPartWithVectorScan] hit primary key cache, and key is {}", cache_key);
+        }
+        else
+        {
+            LOG_DEBUG(log, "[readFromPartWithVectorScan] miss primary key cache for part {}, will load", task->data_part->name);
+
+            /// load pk's bin to memory
+            Columns pk_columns;
+            bool result = readPrimaryKeyBin(pk_columns);
+
+            if (result)
+            {
+                LOG_DEBUG(log, "[readFromPartWithVectorScan] load primary key column and will put into cache");
+                PrimaryKeyCacheManager::getMgr().setPartPkCache(cache_key, std::move(pk_columns));
+                pk_cache_cols_opt = PrimaryKeyCacheManager::getMgr().getPartPkCache(cache_key);
+            }
+            else
+            {
+                LOG_DEBUG(log, "[readFromPartWithVectorScan] failed to load primary key column for part {}, will back to normal read",  task->data_part->name);
+            }
+        }
+
+        if (pk_cache_cols_opt.has_value())
+        {
+            Columns pk_cache_cols = pk_cache_cols_opt.value();
+
+            const auto & primary_key = storage_snapshot->metadata->getPrimaryKey();
+            const size_t pk_col_size = primary_key.column_names.size();
+
+            /// Get pk columns from primary key cache based on mark ranges
+            MutableColumns result_pk_cols;
+            result_pk_cols.resize(pk_col_size);
+            for (size_t i = 0; i < pk_col_size; ++i)
+                result_pk_cols[i] = primary_key.data_types[i]->createColumn();
+
+            /// Check if need to fill _part_offset, will be used for mergeResult with lightweight delete
+            MutableColumnPtr mutable_part_offset_col = nullptr;
+            for (const auto & column_name : non_const_virtual_column_names)
+            {
+                if (column_name == "_part_offset")
+                {
+                    mutable_part_offset_col = ColumnUInt64::create();
+                    break;
+                }
+            }
+
+            MergeTreeRangeReader::ReadResult::ReadRangesInfo read_ranges;
+            const MergeTreeIndexGranularity & index_granularity = task->data_part->index_granularity;
+
+            for (const auto & mark_range : task->mark_ranges)
+            {
+                size_t start_row = index_granularity.getMarkStartingRow(mark_range.begin);
+                size_t stop_row = index_granularity.getMarkStartingRow(mark_range.end);
+
+                read_ranges.push_back({start_row, stop_row - start_row, mark_range.begin, mark_range.end});
+
+                for (size_t i = 0; i < pk_col_size; ++i)
+                    result_pk_cols[i]->insertRangeFrom(*pk_cache_cols[i], start_row, stop_row - start_row);
+
+                if (mutable_part_offset_col)
+                {
+                    auto & data = assert_cast<ColumnUInt64 &>(*mutable_part_offset_col).getData();
+                    while (start_row < stop_row)
+                        data.push_back(start_row++);
+                }
+            }
+
+            Columns result_columns;
+            result_columns.assign(
+                std::make_move_iterator(result_pk_cols.begin()),
+                std::make_move_iterator(result_pk_cols.end())
+                );
+
+            LOG_DEBUG(log, "[readFromPartWithVectorScan] fetch from primary key cache size = {}", result_columns[0]->size());
+
+            /// Get _part_offset if exists.
+            if (mutable_part_offset_col)
+            {
+                part_offset = typeid_cast<const ColumnUInt64 *>(mutable_part_offset_col.get());
+
+                /// _part_offset column exists in original select columns
+                if (!need_remove_part_offset)
+                    result_columns.emplace_back(std::move(mutable_part_offset_col));
+            }
+
+            if (task->vector_scan_manager && task->vector_scan_manager->preComputed())
+            {
+                size_t result_row_num = 0;
+
+                task->vector_scan_manager->mergeResult(
+                    result_columns, /// _Inout_
+                    result_row_num, /// _Out_
+                    read_ranges,
+                    nullptr,
+                    part_offset);
+
+                LOG_DEBUG(log, "[readFromPartImpl] result_columns's size = {}, result_row_num = {}",
+                          result_columns[0]->size(), result_row_num);
+
+                task->mark_ranges.clear();
+                if (result_row_num > 0)
+                {
+                    BlockAndRowCount res = { header_without_virtual_columns.cloneWithColumns(result_columns), result_row_num };
+                    return res;
+                }
+                else /// result_row_num = 0
+                    return {};
+            }
+        }
+    }
+
+    LOG_DEBUG(log, "[readFromPartWithVectorScan] begin read, mark_ranges size = {}", task->mark_ranges.size());
     auto read_result = task->range_reader.read(rows_to_read, task->mark_ranges);
     for (auto it = task->mark_ranges.begin(); it != task->mark_ranges.cend(); ++it)
     {
-        LOG_DEBUG(log, "[readFromPartImpl] mark_range begin = {}, end = {}", it->begin, it->end);
+        LOG_DEBUG(log, "[readFromPartWithVectorScan] mark_range begin = {}, end = {}", it->begin, it->end);
     }
 
     /// All rows were filtered. Repeat.
@@ -211,7 +481,7 @@ MergeTreeBaseSelectProcessor::BlockAndRowCount MergeTreeSelectWithVectorScanProc
 
     UInt64 num_filtered_rows = read_result.numReadRows() - read_result.num_rows;
 
-    LOG_DEBUG(log, "[readFromPartImpl] num_rows: {}, read_rows: {}", read_result.num_rows, read_result.numReadRows());
+    LOG_DEBUG(log, "[readFromPartWithVectorScan] num_rows: {}, read_rows: {}", read_result.num_rows, read_result.numReadRows());
 
     progress({ read_result.numReadRows(), read_result.numBytesRead() });
 
@@ -230,13 +500,16 @@ MergeTreeBaseSelectProcessor::BlockAndRowCount MergeTreeSelectWithVectorScanProc
 
     /// Remove distance_func column from read_result.columns, it will be added by vector search.
     Columns ordered_columns;
-    ordered_columns.reserve(sample_block.columns());
+    if (task->vector_scan_manager)
+        ordered_columns.reserve(sample_block.columns() - 1);
+    else
+        ordered_columns.reserve(sample_block.columns());
     size_t which_cut = 0;
     String vector_scan_col_name;
     for (size_t ps = 0; ps < sample_block.columns(); ++ps)
     {
         auto & col_name = sample_block.getByPosition(ps).name;
-        LOG_DEBUG(log, "[readFromPartImpl]: read column: {}", col_name);
+        LOG_DEBUG(log, "[readFromPartWithVectorScan]: read column: {}", col_name);
         /// TODO: not add distance column to header_without_virtual_columns
         if (isVectorScanFunc(col_name))
         {
@@ -257,20 +530,8 @@ MergeTreeBaseSelectProcessor::BlockAndRowCount MergeTreeSelectWithVectorScanProc
 
     auto read_end_time = std::chrono::system_clock::now();
 
-    LOG_DEBUG(log, "[readFromPartImpl] read time: {}", std::chrono::duration_cast<std::chrono::milliseconds>(read_end_time - read_start_time).count());
+    LOG_DEBUG(log, "[readFromPartWithVectorScan] read time: {}", std::chrono::duration_cast<std::chrono::milliseconds>(read_end_time - read_start_time).count());
 
-
-    if (part_offset)
-    {
-        LOG_DEBUG(log, "[readFromPartImpl] offset values before vector search merge result, and the part name is {}", task->data_part->name);
-        const ColumnUInt64::Container & offset_raw_value = part_offset->getData();
-        const size_t the_size = part_offset->size();
-        for (size_t i = 0; i < the_size && i < 10; ++i)
-        {
-            UInt64 v = offset_raw_value[i];
-            LOG_DEBUG(log, "[readFromPartImpl] offset values --- offset[{}] = {}", i, v);
-        }
-    }
     /// [MQDB] vector search
     if (task->vector_scan_manager && task->vector_scan_manager->preComputed())
     {
@@ -305,7 +566,7 @@ MergeTreeBaseSelectProcessor::BlockAndRowCount MergeTreeSelectWithVectorScanProc
         ColumnWithTypeAndName ctn;
         ctn.column = ordered_columns[i];
 
-        if (i < ordered_columns.size() -1)
+        if (i < ordered_columns.size() - 1)
         {
             size_t src_index = i >= which_cut ? i+1 : i;
             ctn.type = sample_block.getByPosition(src_index).type;
diff --git a/src/Storages/MergeTree/MergeTreeSelectWithVectorScanProcessor.h b/src/Storages/MergeTree/MergeTreeSelectWithVectorScanProcessor.h
index 9b79530a73..d4e1120b2e 100644
--- a/src/Storages/MergeTree/MergeTreeSelectWithVectorScanProcessor.h
+++ b/src/Storages/MergeTree/MergeTreeSelectWithVectorScanProcessor.h
@@ -12,13 +12,19 @@ public:
     template <typename... Args>
     explicit MergeTreeSelectWithVectorScanProcessor(Args &&... args)
         : MergeTreeSelectProcessor{std::forward<Args>(args)...}
-    {}
+    {
+        LOG_TRACE(log, "Reading {} ranges in order from part {}, approx. {} rows starting from {}",
+            all_mark_ranges.size(), data_part->name, total_rows,
+            data_part->index_granularity.getMarkStartingRow(all_mark_ranges.front().begin));
+    }
 
     String getName() const override { return "MergeTreeReadWithVectorScan"; }
 protected:
     BlockAndRowCount readFromPart() override;
     void initializeReadersWithVectorScan();
 
+    bool readPrimaryKeyBin(Columns & out_columns);
+
 private:
     bool getNewTaskImpl() override;
     void finalizeNewTask() override {}
@@ -34,6 +40,9 @@ private:
 
     /// Logic row id for rows, used for vector index scan.
     const ColumnUInt64 * part_offset = nullptr;
+
+    /// True if the query can use primary key cache.
+    bool use_primary_key_cache = false;
 };
 
 }
diff --git a/src/Storages/MergeTree/PrimaryKeyCacheManager.cpp b/src/Storages/MergeTree/PrimaryKeyCacheManager.cpp
index 46fdf7bbe7..e41466b18f 100644
--- a/src/Storages/MergeTree/PrimaryKeyCacheManager.cpp
+++ b/src/Storages/MergeTree/PrimaryKeyCacheManager.cpp
@@ -16,16 +16,7 @@ PrimaryKeyCacheManager::PrimaryKeyCacheManager(size_t max_size)
 void PrimaryKeyCacheManager::setPartPkCache(String part_name, Columns columns)
 {
     /// type of clickhouse LRUCache's value must be std::shard_ptr
-    /// too rigid
-
-    Columns *cols = new Columns(columns.size());
-    for (size_t i = 0; i < columns.size(); ++i)
-    {
-        (*cols)[i] = columns[i];
-    }
-
-    std::shared_ptr<Columns> cols_ptr;
-    cols_ptr.reset(cols);
+    std::shared_ptr<Columns> cols_ptr = std::make_shared<Columns>(columns);
 
     cache_ex.set(part_name, cols_ptr);
 }
@@ -33,26 +24,20 @@ void PrimaryKeyCacheManager::setPartPkCache(String part_name, Columns columns)
 
 std::optional<Columns> PrimaryKeyCacheManager::getPartPkCache(String part_name)
 {
-    std::shared_ptr<Columns> v = cache_ex.get(part_name);
-    if (v == nullptr)
-    {
+    std::shared_ptr<Columns> pk_cache = cache_ex.get(part_name);
+    if (!pk_cache)
         return std::nullopt;
-    }
-    else
-    {
-        return {*v};
-    }
+
+    return *pk_cache;
 }
 
 
-bool PrimaryKeyCacheManager::isSupportedPrimaryKey(const KeyDescription & kd)
+bool PrimaryKeyCacheManager::isSupportedPrimaryKey(const KeyDescription & primary_key)
 {
-    size_t n = kd.data_types.size();
-    if (n != 1)
-    {
+    if (primary_key.data_types.size() != 1)
         return false;
-    }
-    String type_name = kd.data_types[0]->getName();
+
+    String type_name = primary_key.data_types[0]->getName();
     return type_name == "UInt32" || type_name == "UInt64";
 }
 
diff --git a/src/Storages/MergeTree/PrimaryKeyCacheManager.h b/src/Storages/MergeTree/PrimaryKeyCacheManager.h
index 62c51109cb..2550abba3e 100644
--- a/src/Storages/MergeTree/PrimaryKeyCacheManager.h
+++ b/src/Storages/MergeTree/PrimaryKeyCacheManager.h
@@ -18,12 +18,12 @@ class ColumnsWeightFunc
 public:
     size_t operator()(const Columns & cols) const
     {
-        size_t n = 0;
-        for (auto it = cols.begin(); it != cols.cend(); ++it)
+        size_t total_size = 0;
+        for (auto & column : cols)
         {
-            n += (*it)->size();
+            total_size += column->byteSize();
         }
-        return n;
+        return total_size;
     }
 };
 
@@ -35,7 +35,6 @@ public:
     std::optional<Columns> getPartPkCache(String part_name);
 
     /// tools
-
     static bool isSupportedPrimaryKey(const KeyDescription & kd);
 
 
diff --git a/tests/queries/2_vector_search/00016_mqvs_lightweight_delete_with_vector.reference b/tests/queries/2_vector_search/00016_mqvs_lightweight_delete_with_vector.reference
index d84045a545..f36b534c5b 100644
--- a/tests/queries/2_vector_search/00016_mqvs_lightweight_delete_with_vector.reference
+++ b/tests/queries/2_vector_search/00016_mqvs_lightweight_delete_with_vector.reference
@@ -1,4 +1,5 @@
 0
+0
 0	[0,0,0]	0.030000001
 1	[1,1,1]	2.4299998
 3	[3,3,3]	25.230003
diff --git a/tests/queries/2_vector_search/00016_mqvs_lightweight_delete_with_vector.sql b/tests/queries/2_vector_search/00016_mqvs_lightweight_delete_with_vector.sql
index 8b0933f673..e5bcf21149 100644
--- a/tests/queries/2_vector_search/00016_mqvs_lightweight_delete_with_vector.sql
+++ b/tests/queries/2_vector_search/00016_mqvs_lightweight_delete_with_vector.sql
@@ -12,6 +12,8 @@ set mutations_sync=1;
 
 delete from test_vector where id = 2;
 
+SELECT sleep(2);
+
 SELECT id, vector, distance('topK=10')(vector, [0.1, 0.1, 0.1]) FROM test_vector;
 
 drop table test_vector;
diff --git a/tests/queries/2_vector_search/00021_mqvs_support_primary_key_cache.reference b/tests/queries/2_vector_search/00021_mqvs_support_primary_key_cache.reference
new file mode 100644
index 0000000000..e51c223f07
--- /dev/null
+++ b/tests/queries/2_vector_search/00021_mqvs_support_primary_key_cache.reference
@@ -0,0 +1,12 @@
+0
+0	[0,0,0]	0.030000001
+1	[1,1,1]	2.4299998
+2	[2,2,2]	10.83
+3	[3,3,3]	25.230003
+4	[4,4,4]	45.630005
+0
+0	[0,0,0]	0.030000001
+1	[1,1,1]	2.4299998
+2	[2,2,2]	10.83
+4	[4,4,4]	45.630005
+5	[5,5,5]	72.03
diff --git a/tests/queries/2_vector_search/00021_mqvs_support_primary_key_cache.sql b/tests/queries/2_vector_search/00021_mqvs_support_primary_key_cache.sql
new file mode 100644
index 0000000000..bb84d045d9
--- /dev/null
+++ b/tests/queries/2_vector_search/00021_mqvs_support_primary_key_cache.sql
@@ -0,0 +1,18 @@
+DROP TABLE IF EXISTS test_pk_cache;
+CREATE TABLE test_pk_cache(id Int32, vector FixedArray(Float32, 3)) engine MergeTree primary key id SETTINGS index_granularity=1024, min_rows_to_build_vector_index=1000, enable_primary_key_cache=true;
+INSERT INTO test_pk_cache SELECT number, [number, number, number] FROM numbers(2100);
+
+ALTER TABLE test_pk_cache ADD VECTOR INDEX v1 vector TYPE IVFFLAT;
+SELECT sleep(2);
+
+SELECT id, vector, distance('topK=5')(vector, [0.1, 0.1, 0.1]) FROM test_pk_cache;
+
+set allow_experimental_lightweight_delete=1;
+set mutations_sync=1;
+
+delete from test_pk_cache where id = 3;
+SELECT sleep(1);
+
+SELECT id, vector, distance('topK=5')(vector, [0.1, 0.1, 0.1]) FROM test_pk_cache;
+
+drop table test_pk_cache;
-- 
2.32.1 (Apple Git-133)

