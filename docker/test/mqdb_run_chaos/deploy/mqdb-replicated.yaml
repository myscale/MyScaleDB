---
# Setup Service to provide access to ClickHouse keeper for clients
apiVersion: v1
kind: Service
metadata:
  # DNS would be like clickhouse-keeper.namespace.svc
  name: clickhouse-keeper
  namespace: "CHAOS_NAMESPACE"
  labels:
    app: clickhouse-keeper
spec:
  ports:
    - port: 2181
      name: client
    - port: 7000
      name: prometheus
  selector:
    app: clickhouse-keeper
    what: node
---
# Setup Headless Service for StatefulSet
apiVersion: v1
kind: Service
metadata:
  # DNS would be like clickhouse-keeper-0.clickhouse-keepers.namespace.svc
  name: clickhouse-keepers
  namespace: "CHAOS_NAMESPACE"
  labels:
    app: clickhouse-keeper
spec:
  ports:
    - port: 9444
      name: raft
  clusterIP: None
  selector:
    app: clickhouse-keeper
    what: node
---
# Setup max number of unavailable pods in StatefulSet
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: clickhouse-keeper-pod-disruption-budget
  namespace: "CHAOS_NAMESPACE"
spec:
  selector:
    matchLabels:
      app: clickhouse-keeper
  maxUnavailable: 1
---
# Setup ClickHouse Keeper settings
apiVersion: v1
kind: ConfigMap
metadata:
  name: clickhouse-keeper-settings
  namespace: "CHAOS_NAMESPACE"
data:
  keeper_config.xml: |
    <clickhouse>
        <include_from>/tmp/clickhouse-keeper/config.d/generated-keeper-settings.xml</include_from>
        <logger>
            <level>information</level>
            <console>true</console>
        </logger>
        <listen_host>0.0.0.0</listen_host>
        <keeper_server incl="keeper_server">
            <path>/var/lib/clickhouse-keeper</path>
            <tcp_port>2181</tcp_port>
            <four_letter_word_white_list>*</four_letter_word_white_list>
            <coordination_settings>
                <!-- <raft_logs_level>trace</raft_logs_level> -->
                <raft_logs_level>information</raft_logs_level>
            </coordination_settings>
        </keeper_server>
        <prometheus>
            <endpoint>/metrics</endpoint>
            <port>7000</port>
            <metrics>true</metrics>
            <events>true</events>
            <asynchronous_metrics>true</asynchronous_metrics>
            <!-- https://github.com/ClickHouse/ClickHouse/issues/46136 -->
            <status_info>false</status_info>
        </prometheus>
    </clickhouse>

---
# Setup ClickHouse Keeper StatefulSet
apiVersion: apps/v1
kind: StatefulSet
metadata:
  # nodes would be named as clickhouse-keeper-0, clickhouse-keeper-1, clickhouse-keeper-2
  name: clickhouse-keeper
  namespace: "CHAOS_NAMESPACE"
  labels:
    app: clickhouse-keeper
spec:
  selector:
    matchLabels:
      app: clickhouse-keeper
  serviceName: clickhouse-keepers
  replicas: 3
  updateStrategy:
    type: RollingUpdate
  podManagementPolicy: Parallel
  template:
    metadata:
      labels:
        app: clickhouse-keeper
        what: node
      annotations:
        prometheus.io/port: '7000'
        prometheus.io/scrape: 'true'
    spec:
      volumes:
        - name: clickhouse-keeper-settings
          configMap:
            name: clickhouse-keeper-settings
            items:
              - key: keeper_config.xml
                path: keeper_config.xml
      containers:
        - name: clickhouse-keeper
          imagePullPolicy: IfNotPresent
          image: "clickhouse/clickhouse-keeper:23-alpine"
          resources:
            requests:
              memory: "100Mi"
              cpu: "25m"
            limits:
              memory: "400Mi"
              cpu: "200m"
          volumeMounts:
            - name: clickhouse-keeper-settings
              mountPath: /etc/clickhouse-keeper/
            - name: clickhouse-keeper-datadir-volume
              mountPath: /var/lib/clickhouse-keeper
          env:
            - name: SERVERS
              value: "3"
            - name: RAFT_PORT
              value: "9444"
          command:
            - bash
            - -x
            - -c
            - |
              HOST=`hostname -s` &&
              DOMAIN=`hostname -d` &&
              if [[ $HOST =~ (.*)-([0-9]+)$ ]]; then
                  NAME=${BASH_REMATCH[1]}
                  ORD=${BASH_REMATCH[2]}
              else
                  echo "Failed to parse name and ordinal of Pod"
                  exit 1
              fi &&
              export MY_ID=$((ORD+1)) &&
              mkdir -p /tmp/clickhouse-keeper/config.d/ &&
              {
                echo "<yandex><keeper_server>"
                echo "<server_id>${MY_ID}</server_id>"
                echo "<raft_configuration>"
                for (( i=1; i<=$SERVERS; i++ )); do
                    echo "<server><id>${i}</id><hostname>$NAME-$((i-1)).${DOMAIN}</hostname><port>${RAFT_PORT}</port></server>"
                done
                echo "</raft_configuration>"
                echo "</keeper_server></yandex>"
              } > /tmp/clickhouse-keeper/config.d/generated-keeper-settings.xml &&
              cat /tmp/clickhouse-keeper/config.d/generated-keeper-settings.xml && 
              clickhouse-keeper --config-file=/etc/clickhouse-keeper/keeper_config.xml
          livenessProbe:
            exec:
              command:
                - bash
                - -xc
                - 'date && OK=$(exec 3<>/dev/tcp/127.0.0.1/2181 ; printf "ruok" >&3 ; IFS=; tee <&3; exec 3<&- ;); if [[ "$OK" == "imok" ]]; then exit 0; else exit 1; fi'
            initialDelaySeconds: 20
            timeoutSeconds: 15
          ports:
            - containerPort: 7000
              name: prometheus
  volumeClaimTemplates:
    - metadata:
        name: clickhouse-keeper-datadir-volume
      spec:
        accessModes:
          - ReadWriteOnce
        resources:
          requests:
            storage: 10Gi
---
apiVersion: "clickhouse.altinity.com/v1"
kind: "ClickHouseInstallation"
metadata:
  name: "chaos-test"
  namespace: "CHAOS_NAMESPACE"
spec:
  defaults:
    templates:
      serviceTemplate: service-template
      replicaServiceTemplate: replica-service-template
      podTemplate: pod-template
      dataVolumeClaimTemplate: data-volume-claim-template
      logVolumeClaimTemplate: log-volume-claim-template

  configuration:
    zookeeper:
      nodes:
        - host: clickhouse-keeper
          port: 2181

    settings:
      disable_internal_dns_cache: 1
      logger/level: information
      logger/size: 1000M
      logger/count: 3
      vector_index_cache_path: /var/lib/clickhouse/vector_index_cache
      mark_cache_size: 209715200
      uncompressed_cache_size: 314572800
      max_parts_in_total: 10000
      max_concurrent_queries: 1000
      max_table_size_to_drop: 1000000000000
      query_log/database: system
      query_log/flush_interval_milliseconds: "7500"
      query_log/partition_by: toYYYYMMDD(event_date)
      query_log/table: query_log
      query_log/ttl: event_date + INTERVAL 3 DAY DELETE
      merge_tree/number_of_free_entries_in_pool_to_execute_mutation: 2
      merge_tree/number_of_free_entries_in_pool_to_lower_max_size_of_merge: 2
      merge_tree/max_parts_to_merge_at_once: 100
      merge_tree/merge_max_block_size: 256
      merge_tree/old_parts_lifetime: 5
      merge_tree/default_mstg_disk_mode: 1
      merge_tree/max_bytes_to_merge_at_max_space_in_pool: 5368709120
      merge_tree/simple_merge_selector_base: 1.2
      allow_experimental_transactions: 42
      transactions_info_log/database: system
      transactions_info_log/table: transactions_info_log
      transactions_info_log/flush_interval_milliseconds: 7500
    users:
      default/networks/ip:
        - "127.0.0.1"
        - "::1"
        - "0.0.0.0/0"
    profiles:
      default/allow_experimental_lightweight_delete: "true"
      default/mutations_sync: "1"
      default/log_queries_cut_to_length: "100"
      default/log_queries_min_type: QUERY_FINISH
      default/log_query_threads: 0
      default/log_query_views: 0
      default/background_pool_size: 4
      default/background_merges_mutations_concurrency_ratio: 2
      default/min_insert_block_size_bytes: 33554432
      default/allow_experimental_database_replicated: "true"
      default/database_replicated_always_convert_table_to_replicated: "true"
      default/database_replicated_allow_replicated_engine_arguments: "false"
      default/database_replicated_default_zk_path_prefix: "/clickhouse/{installation}/{cluster}/databases/"
      default/default_database_engine: "Replicated"
      default/default_table_engine: "ReplicatedMergeTree"
      default/database_replicated_always_execute_with_on_cluster: "true"
      default/database_replicated_default_cluster_name: "{cluster}"
      default/database_replicated_allow_explicit_arguments: "false"
      default/async_insert: 1
      default/allow_experimental_object_type: "true"

    clusters:
      - name: clickhouse
        layout:
          shardsCount: 1
          replicasCount: 1
    files:
      z_log_disable.xml: |-
        <?xml version="1.0"?>
        <clickhouse>
            <asynchronous_metric_log remove="1"/>
            <session_log remove="1"/>
            <text_log remove="1" />
            <trace_log remove="1"/>
            <query_views_log remove="1" />
            <query_thread_log remove="1" />
        </clickhouse>
  templates:
    serviceTemplates:
      - name: service-template
        metadata:
          annotations:
            service.kubernetes.io/qcloud-loadbalancer-internal-subnetid: subnet-itzzzhyz
        spec:
          type: LoadBalancer
          ports:
            - name: http
              port: 8123
              protocol: TCP
              targetPort: http
            - name: tcp
              port: 9000
              protocol: TCP
              targetPort: tcp
      - name: replica-service-template
        spec:
          type: ClusterIP
          clusterIP: None
          ports:
            - name: http
              port: 8123
              protocol: TCP
              targetPort: http
            - name: tcp
              port: 9000
              protocol: TCP
              targetPort: tcp
            - name: interserver
              port: 9009
              protocol: TCP
              targetPort: interserver
            - name: metrics
              port: 9363
              protocol: TCP
              targetPort: metrics
    podTemplates:
      - name: pod-template
        spec:
          nodeSelector:
            node.kubernetes.io/localpath-volume: "true"
          tolerations:
            - key: node.kubernetes.io/localpath-volume
              value: "true"
              effect: NoExecute
          containers:
            - name: clickhouse
              image: harbor.internal.moqi.ai/mqdb/mqdb:BASE_IMAGE
              securityContext:
                privileged: true
              command:
                - bash
                - -c
                - ulimit -l unlimited && /entrypoint.sh
              ports:
                - containerPort: 9000
                  name: tcp
                  protocol: TCP
                - containerPort: 8123
                  name: http
                  protocol: TCP
                - containerPort: 9009
                  name: interserver
                  protocol: TCP
                - containerPort: 9363
                  name: metrics
                  protocol: TCP
              resources:
                requests:
                  cpu: 1000m
                  memory: 3Gi
                limits:
                  cpu: 3000m
                  memory: 4Gi
              volumeMounts:
                - mountPath: /var/lib/clickhouse/vector_index_cache
                  name: local-volume-claim-template

    volumeClaimTemplates:
      - name: data-volume-claim-template
        spec:
          accessModes:
            - ReadWriteOnce
          resources:
            requests:
              storage: 80Gi
      - name: log-volume-claim-template
        spec:
          accessModes:
            - ReadWriteOnce
          resources:
            requests:
              storage: 10Gi
      - name: local-volume-claim-template
        reclaimPolicy: Delete
        spec:
          accessModes:
            - ReadWriteOnce
          resources:
            requests:
              storage: 100Gi
          storageClassName: localpath